{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "from itertools import chain\n",
    "root_dir = \"/pfs/work7/workspace/scratch/tu_zxobe27-master_thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "rivers = gpd.read_feather(f\"{root_dir}data/river_network/shapefile.feather\")\n",
    "drainage_polygons = gpd.read_feather(f\"{root_dir}data/drainage/extracted_drainage_polygons.feather\")\n",
    "land_cover = dd.read_parquet(f\"{root_dir}data/land_cover/temp_extracted_land_cover/\", columns=[\"year\", \"deforestation\", \"deforestation_p\", \"deforestation_a\", \"deforestation_u\", \"deforestation_m\", \"forest\", \"pasture\", \"agriculture\", \"urban\", \"mining\", \"total\"]).astype(np.uint32)\n",
    "cloud_cover = dd.read_parquet(f\"{root_dir}data/cloud_cover/cloud_cover.parquet\").astype({\"grid_id\": \"uint32\", \"index\": \"uint32\", \"year\": \"uint32\", \"cloud_cover\": \"float32\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare deforestation data in dask dataframe\n",
    "t_deforestation = land_cover.groupby([\"grid_id\", \"index\", \"year\"]).sum().reset_index().astype(np.uint32).persist()\n",
    "del land_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregation within adm2 regions\n",
    "\n",
    "# prepare a table from adm2 to grid_id\n",
    "adm2_table = pd.merge(\n",
    "    rivers[[\"adm2\", \"estuary\", \"river\", \"segment\", \"subsegment\"]], \n",
    "    drainage_polygons[[\"estuary\", \"river\", \"segment\", \"subsegment\"]].reset_index(names = [\"grid_id\", \"index\"]), \n",
    "    on=[\"estuary\", \"river\", \"segment\", \"subsegment\"], how=\"right\",\n",
    "    ).dropna()[[\"grid_id\", \"index\", \"adm2\"]].astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge deforestation data with adm2\n",
    "t_deforestation_adm = dd.merge(t_deforestation, dd.from_pandas(adm2_table, npartitions=1), on=[\"grid_id\", \"index\"], how=\"left\")\n",
    "# calculate deforestation data for each adm2\n",
    "c_final_df_deforestation = t_deforestation_adm.drop(columns=[\"grid_id\", \"index\"]).groupby([\"adm2\", \"year\"]).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge cloud cover data with adm2\n",
    "t_cloud_cover_adm = dd.merge(cloud_cover, dd.from_pandas(adm2_table, npartitions=1), on=[\"grid_id\", \"index\"], how=\"left\")\n",
    "# calculate cloud cover data for each adm2\n",
    "c_final_df_cloud_cover = t_cloud_cover_adm.drop(columns=[\"grid_id\", \"index\"]).groupby([\"adm2\", \"year\"]).mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge deforestation and cloud cover data\n",
    "out_df = pd.merge(c_final_df_deforestation, c_final_df_cloud_cover, on=[\"adm2\", \"year\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write GID\n",
    "out_df = out_df.reset_index().astype({\"year\": np.int16})\n",
    "out_df[\"municipality\"] = out_df.adm2.map(id_gid_lookup)\n",
    "out_df.drop(columns=[\"adm2\"], inplace=True)\n",
    "\n",
    "# save to parquet\n",
    "out_df.to_parquet(f\"{root_dir}data/deforestation/deforestation.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## aggregation in upstream nodes\n",
    "# compute lookup from node to list of polygons\n",
    "drainage_polygons_tmp = drainage_polygons[((~ drainage_polygons.is_empty) & (~ drainage_polygons.geometry.isna()))].drop(columns=\"geometry\").reset_index(names = [\"grid\", \"index\"])\n",
    "drainage_polygons_tmp = pd.merge(drainage_polygons_tmp, rivers.drop(columns = [\"NORIOCOMP\", \"CORIO\", \"geometry\"]), on = [\"estuary\", \"river\", \"segment\", \"subsegment\"])\n",
    "drainage_polygons_tmp[\"identifier\"] = drainage_polygons_tmp.apply(lambda x: [x[\"grid\"], x[\"index\"]], axis = 1)\n",
    "node_polygon_lookup = drainage_polygons_tmp.set_index(\"upstream_node_id\").groupby(level=0).apply(lambda x: x[\"identifier\"].tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reachability data\n",
    "reachability_municipalities = json.load(open(\"/pfs/work7/workspace/scratch/tu_zxobe27-master_thesis/data/river_network/reachability_municipalities.json\", \"r\"))\n",
    "\n",
    "## convert to integer ID\n",
    "# load municipalities\n",
    "municipalities = gpd.read_file(\"/pfs/work7/workspace/scratch/tu_zxobe27-master_thesis/data/misc/raw/gadm/gadm41_BRA_2.json\", engine=\"pyogrio\")\n",
    "# create mapping from GID_2 to integer ID\n",
    "gid_id_lookup = municipalities[\"GID_2\"].reset_index().set_index(\"GID_2\")[\"index\"].to_dict()\n",
    "id_gid_lookup = municipalities[\"GID_2\"].to_dict()\n",
    "# convert reachability_municipalities keys to integer ID\n",
    "reachability_municipalities = {gid_id_lookup.get(k, k): v for k, v in reachability_municipalities.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chunk all municipalities into chunks if 1M nodes\n",
    "# assign chunks of 1M nodes\n",
    "t_chunks = np.cumsum([len(x) if x is not None else 0 for i, x in reachability_municipalities.items()]) // 5e5\n",
    "# get indices of chunks\n",
    "t_chunks = [(int(np.argmax(t_chunks == i)), int(len(t_chunks) - np.argmax(t_chunks[::-1] == i) - 1)) for i in np.unique(t_chunks)]\n",
    "# get nodes split into chunks\n",
    "c_node_ids = [{y: reachability_municipalities[y] for y in list(reachability_municipalities.keys())[x[0]:x[1]]} for x in t_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over chunks\n",
    "out_df = [None] * len(c_node_ids)\n",
    "for i in [0]:#range(len(c_node_ids)):\n",
    "    ## prepare data frame for final estimation\n",
    "    # get polygon ids\n",
    "    t_index_prep = {(key, int(value)): node_polygon_lookup.get(value, [None, None]) for key, values in c_node_ids[i].items() if values is not None for value in values}\n",
    "    # combine in tuple for index\n",
    "    t_index_prep = [(key[0], key[1], int(value[0]), int(value[1])) for key, values in t_index_prep.items() for value in values if value is not None]         \n",
    "    # create dataframe with indices\n",
    "    c_final_df = dd.from_pandas(pd.DataFrame().from_records(t_index_prep, columns = [\"municipality\", \"node\", \"grid_id\", \"index\"])).astype(np.uint32)\n",
    "    # merge with deforestation data and summarize\n",
    "    c_final_df_deforestation = dd.merge(c_final_df, t_deforestation, on = [\"grid_id\", \"index\"], how = \"left\")\n",
    "    c_final_df_deforestation = c_final_df_deforestation.drop(columns=[\"grid_id\", \"index\", \"node\"]).groupby([\"municipality\", \"year\"]).sum().compute().astype(np.float32)\n",
    "    # merge with cloud cover data and summarize\n",
    "    c_final_df_cloud_cover = dd.merge(c_final_df, cloud_cover, on = [\"grid_id\", \"index\"], how = \"left\")\n",
    "    c_final_df_cloud_cover = c_final_df_cloud_cover.groupby([\"municipality\", \"year\"]).agg({\"cloud_cover\": \"mean\"}).compute().astype(np.float32)\n",
    "    # Group by the bins and sum the value column\n",
    "    #agg_dict = {\"deforestation\": \"sum\", \"deforestation_p\": \"sum\", \"deforestation_a\": \"sum\", \"deforestation_u\": \"sum\", \"deforestation_m\": \"sum\", \"forest\": \"sum\", \"pasture\": \"sum\", \"agriculture\": \"sum\", \"urban\": \"sum\", \"mining\": \"sum\", \"total\": \"sum\"}\n",
    "    out_df[i] = pd.merge(c_final_df_deforestation, c_final_df_cloud_cover, on = [\"municipality\", \"year\"], how = \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all chunks\n",
    "out_df = pd.concat(out_df).reset_index().astype({\"year\": np.int16})\n",
    "# get GID_2\n",
    "out_df[\"municipality\"] = out_df.municipality.map(id_gid_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to parquet\n",
    "out_df.to_parquet(f\"{root_dir}data/land_cover/land_cover_municipalities_agg.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
